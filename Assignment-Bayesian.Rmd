---
title: "Bayesian Statistics"
author: "Kalyango Jovan"
date: "26/02/2022"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```



```{r}
lm.bayes <- function(y, x, tau.a, tau.b, alpha = 0.001, beta = 0.001, niter = 5000) {
n <- length(y)
a <- mean(y)
b <- 0
tau <- 1
result <- matrix(nrow = niter, ncol = 3)
for (i in 1:niter) {
a <- rnorm(1, mean = (tau/(n * tau + tau.a)) * sum(y - b * x), sd = 1/sqrt(n * tau +
tau.a))
b <- rnorm(1, mean = (tau * sum((y - a) * x))/(tau * sum(x^2) + tau.b), sd = 1/sqrt(tau *
sum(x^2) + tau.b))
tau <- rgamma(1, shape = alpha + n/2, rate = beta + 0.5 * sum((y - a - b * x)^2))
result[i, ] <- c(a, b, tau)
}
result
}
```


```{r}
#houses
growth.lm=lm.bayes(y=houses[,5], x=houses[,c(2:4)], tau.a=0.001, tau.b=0.001, niter=50000)

```

```{r}
houses$x2
```


```{r}
sample_tau <- function(ys, alpha, beta, alpha0, beta0) {
  rgamma(1,
    shape = alpha0 + nrow(ys) / 2,
    rate = beta0 + 0.5 * sum((ys$y - (alpha + as.matrix(ys$x) %*% beta))^2)
  )
}

sample_alpha <- function(ys, beta, tau, mu0, tau0) {
  prec <- tau0 + tau * nrow(ys)
  mean <- (tau0 + tau * sum(ys$y - as.matrix(ys$x)%*% beta)) / prec
  rnorm(1, mean = mean, sd = 1 / sqrt(prec))
}

sample_beta <- function(ys, alpha, tau, mu0, tau0) {
  prec <- tau0 + tau * sum(ys$x*ys$x)
  mean <- (tau0 + tau * sum((ys$y - alpha) * ys$x)) / prec
  rnorm(1, mean = mean, sd = 1 / sqrt(prec))
}
```


```{r}
gibbs_sample <- function(ys,
                         tau0,
                         alpha0,
                         beta0,
                         m,
                         alpha_tau,
                         beta_tau,
                         mu_alpha,
                         tau_alpha,
                         mu_beta,
                         tau_beta) {
  tau <- numeric(m)
  alpha <- numeric(m)
  beta <- numeric(m)
  
  tau[1] <- tau0
  
  alpha[1] <- alpha0
  beta[1] <- beta0
  
  for (i in 2:m) {
    tau[i] <-
      sample_tau(ys, alpha[i - 1], beta[i - 1], alpha_tau, beta_tau)
    alpha[i] <-
      sample_alpha(ys, beta[i - 1], tau[i], mu_alpha, tau_alpha)
    beta[i] <- sample_beta(ys, alpha[i], tau[i], mu_beta, tau_beta)
  }
  
  tibble(iteration = seq_len(m),
         tau,
         alpha,
         beta)
}

ys <- tibble(y = houses$Price, 
             x = houses$Size)

#plan(multiprocess)
#iters <- future_map_dfr(
  #.x = 1:4,
  #.f = function(x) 
    gibbs_sample(
      ys,
      tau0 = 0.5,
      alpha0 = 60,
      beta0 = 0.3,
      m = 1e4,
      alpha_tau = 3,
      beta_tau = 2,
      mu_alpha = 0,
      tau_alpha = 0.01,
      mu_beta = 0,
      tau_beta = 0.01
    )
  #.id = "chain"
```


```{r}
```{r}
summary_DIC <-
    dic%>% 
     select(mean_deviance:dic) %>% 
      summarise(((`M1` = formula =Price~Taxes+Beds+Baths , data = houses, 
n_iter = 10000,
    starting_values),
               `M2` = formula =Price~Taxes+Beds, data = houses,
n_iter = 10000,
    starting_values)),
   `M3` =formula =Price~Taxes+Beds+Size, 
    data = houses, 
n_iter = 10000,
    starting_values),
                `M4` =formula =Price~Taxes+Beds*Size, 
    data = houses, 
n_iter = 10000,
    starting_values)))

rownames(summary_DIC) <- c("mean_deviance", "complexity
", "dic")
knitr::kable(round(summary_quantiles, 3), caption = "Posterior Quantiles")
```
```


The goal of Bayesian inference is to maintain a full posterior probability distribution over a set of random variables.However, maintaining and using this distribution often involves computing integrals which, for most non-trivial models, is intractable. Sampling algorithms based on Monte Carlo Markov Chain(MCMC) techniques are one possible way to go about inference in such models


```{r}
Because of lack of prior information about the dataset of my choice, improper priors are used to analyze models. The posterior distributions for these priors may not be proper. However, the conjugate improper priors may allow for construction Gibbs sampler. Thus the Gibbs chains can be used to analyze the posterior distributions without having any information of posterior properties. This may lead to some serious problems. In this linear model, improper priors are used, which leads to a improper posterior distribution. However, the Gibbs sampler still provide some perfectly reasonable result. In this case, users may not be aware of the impropriety. So users must demonstrate the propriety before using the Markov chain Monte Carlo techniques.Here I investigated the *frequentist linear model*  and *Bayesian regression* model by comparing the effect of improper priors on Gibbs sampler. I use *uninformative* priors in this case,because I have no clear prior and/or likelihood of the model parameters. The analysis in the report aim at accessing the predictors' impact on Price of house using a Bayesian  multivariate approach to program the Gibbs-Sampler that is explained in [@article{gelfand2000gibbs]. To run a Gibbs sampler we have to specify priors and likelihood to get the posterior which is of the interest. The algorithm starts when we make some assumptions on parameters of interest i.e ${\beta}$ and $\sigma^2$ are considered to be independent, hence $p({\beta}, \sigma^2) =p({\beta})p(\sigma^2)$. Posterior distribution is a product of priors and likelihood. 
If the prior and likelihood are known for all hypotheses, then Bayes' formula$P({A|B})=P({B|A})*P({A})/P({B})$ computes the posterior exactly.We call this the deductive logic of probability theory, and it gives
a direct way to compare hypotheses, draw conclusions, and make decisions. However, in this case,the prior probabilities on hypotheses are not known, the recourse is the art of statistical inference: we either make up a prior (Bayesian way ) or do our best using only the likelihood (frequentist way). 

```



In this assignment, I will still use the dataset I used last year, this is because it was not the problem of my disallowed sweat last year. The dataset is collected on 27 family houses. 
The sale price (in thousands of dollars) and the floor size (in squared meters) for a random sample of 27 single-family houses sold in a given town during 2005 are stored in the file house2.txt.In particular, for each house, the file contains:Taxes the amount of property taxes (thousands of dollars), paid by the previous owner during 2004,Beds number of bedrooms,Baths number of bathrooms,Size floor size (squared meters),Price sale price (thousands of dollars).It has a rows-by-columns structure, where each row corresponds to a unit, each column to a statistical variable. Differently from a matrix,it can contain strings, numerical and logical values (thus allowing for the presence of both categorical and numerical variables). For example, it is possible to visualize the first 6 rows of seeds using the following instruction: house2[1:6,]. It is also organized as a list: each element of the list corresponds to a variable: 

The aim of the report is to fit a multiple linear regression model with Bayesian parameter estimation, in order to assess the impact of the four predictors on the sale price.Furthermore, I aim to know which of the predictors has a larger impact on the Price of the house, in other words, finding the most important or useful predictor(s). The research questions are to be answered using Bayesian statistical analysis techniques such as Gibb's sampler and metropolis-Hastings algorithm as a data set of my choice fits the analysis type.

The research questions will be responded to in a subsequently, test whether the first model (containing all three predictors) fits the data better than a model 
that excludes some predictor(s). 